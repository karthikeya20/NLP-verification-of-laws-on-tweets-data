{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SimpleRNN, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"https://raw.githubusercontent.com/ryanmcdermott/trump-speeches/master/speeches.txt\")\n",
    "data = r.text\n",
    "\n",
    "train_data=data[:723135]\n",
    "#80%\n",
    "\n",
    "test_data=data[723135:]\n",
    "#20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaner(data):\n",
    "\tf=data.replace(u'\\xa0', u' ')\n",
    "\tf=re.sub(r'\\n', ' ', f)\n",
    "\tf=re.sub(r'\\r', ' ', f)\n",
    "\tl = sent_tokenize(f)\n",
    "\tl=list(filter(None, l))\n",
    "\treturn l\n",
    "data_test=[]\n",
    "final1=data_cleaner(test_data)\n",
    "for i in final1:\n",
    "    s = i.lower()\n",
    "    s = re.sub(r\"[^a-zA-Z0-9'\\s]\", ' ', s)\n",
    "    s='<s> '+s+' </s>'\n",
    "    data_test.append(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counter(n_gram):\n",
    "\td=dict()\n",
    "\tfor i in n_gram:\n",
    "\t\tif i in d:\n",
    "\t\t\td[i]+=1\n",
    "\t\telse:\n",
    "\t\t\td[i]=1\n",
    "\treturn(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_finder(n1_gram,n1_gram_count,n0_gram_count):\n",
    "\t# Along with Add-1 smoothing\n",
    "\tle=len(n1_gram_count)\n",
    "\ttotal_n_grams=len(n1_gram)\n",
    "\tunique_grams=list(n1_gram_count.keys())\n",
    "\tprob_vec=[0 for i in range(le)]\n",
    "\tif len(n1_gram[0])==1:\n",
    "\t\tfor i in range(le):\n",
    "\t\t\tprob_vec[i]=(n1_gram_count[unique_grams[i]])/total_n_grams\n",
    "\telif len(n1_gram[0])==2:\n",
    "\t\tfor i in range(le):\n",
    "\t\t\tprob_vec[i]=n1_gram_count[unique_grams[i]]/n0_gram_count[(unique_grams[i][0],)]\n",
    "\telif len(n1_gram[0])==3:\n",
    "\t\tfor i in range(le):\n",
    "\t\t\tprob_vec[i]=n1_gram_count[unique_grams[i]]/n0_gram_count[(unique_grams[i][0],unique_grams[i][1])]\n",
    "\telif len(n1_gram[0])==4:\n",
    "\t\tfor i in range(le):\n",
    "\t\t\tprob_vec[i]=n1_gram_count[unique_grams[i]]/n0_gram_count[(unique_grams[i][0],unique_grams[i][1],unique_grams[i][2])]\n",
    "\treturn(unique_grams,prob_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data=data_cleaner(train_data)\n",
    "unigrams=[]\n",
    "bigrams=[]\n",
    "trigrams=[]\n",
    "quadgrams=[]\n",
    "data_train=[]\n",
    "for i in final_data:\n",
    "    s = i.lower()\n",
    "    s = re.sub(r\"[^a-zA-Z0-9'\\s]\", ' ', s)\n",
    "    s='<s> '+s+' </s>'\n",
    "    data_train.append(s)\n",
    "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "    unigrams+=list(ngrams(tokens, 1))\n",
    "    bigrams+=list(ngrams(tokens, 2))\n",
    "    trigrams+=list(ngrams(tokens, 3))\n",
    "    quadgrams+=list(ngrams(tokens, 4))\n",
    "unigram_count=counter(unigrams)\n",
    "bigram_count=counter(bigrams)\n",
    "trigram_count=counter(trigrams)\n",
    "quadgram_count=counter(quadgrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 sentences by unigram\n",
      "\n",
      "and that the great. \n",
      "you so he. \n",
      "the. \n",
      "</s> \n",
      "</s> \n",
      "\n",
      "\n",
      "5 sentences by bigram\n",
      "\n",
      "so unfair. \n",
      "i could be a real estate. \n",
      "we re doing this and i m going to be allowed to tell you hear a big fat ugly bubble that are so it. \n",
      "i have to make the way it. \n",
      "i love mexico. \n",
      "\n",
      "\n",
      "5 sentences by trigram\n",
      "\n",
      "jerry falwell jr endorsed me. \n",
      "stay there go in and get them to come in and get him back. \n",
      "you know what you ve got to be an election he should have gotten if i lose i ll tell you. \n",
      "politicians are all talk no action. \n",
      "i'm not getting millions and millions of dollars from all these other guys they were forced to make our country. \n",
      "\n",
      "\n",
      "5 sentences by quadgram\n",
      "\n",
      "finally america no longer has a clear understanding of our foreign policy goals. \n",
      "it' can't be mitt because mitt ran and failed. \n",
      "but you know what. \n",
      "and it s been a very exciting process. \n",
      "my patients are beside themselves. \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_prob=prob_finder(unigrams,unigram_count,[])\n",
    "bi_prob=prob_finder(bigrams,bigram_count,unigram_count)\n",
    "tri_prob=prob_finder(trigrams,trigram_count,bigram_count)\n",
    "quad_prob=prob_finder(quadgrams,quadgram_count,trigram_count)\n",
    "# each of these is a list --> [all_unique_n_grams,corresponding_probabilities] \n",
    "req_strings=[]\n",
    "def unigram_generator(uni_prob):\n",
    "# unigram sentence generator\n",
    "\tprint(\"5 sentences by unigram\")\n",
    "\tprint()\n",
    "\tfor i in range(5):\n",
    "\t\ts1out=''\n",
    "\t\twhile '</s>' not in s1out :\n",
    "\t\t\trand=np.random.multinomial(15,uni_prob[1])\n",
    "\t\t\tma=np.argmax(rand)\n",
    "\t\t\ts1out+=uni_prob[0][ma][0]+' '\n",
    "\t\ts1out=s1out.replace('<s> ','')\n",
    "\t\tprint(s1out.replace(' </s>','.'))\n",
    "\tprint()\n",
    "\tprint()\n",
    "\treturn(1)\n",
    "\n",
    "\n",
    "def bigram_generator(bi_prob):\n",
    "\tprint(\"5 sentences by bigram\")\n",
    "\tprint()\n",
    "\n",
    "\t# bigram sentence generator\n",
    "\tfor m in range(5):\n",
    "\t\ts2out=''\n",
    "\t\tstart='<s>'\n",
    "\t\twhile '</s>' not in s2out :\n",
    "\t\t\treqlist=[]\n",
    "\t\t\treq_probs=[]\n",
    "\t\t\tfor i in range(len(bi_prob[0])):\n",
    "\t\t\t\tif bi_prob[0][i][0]==start:\n",
    "\t\t\t\t\treqlist.append(bi_prob[0][i])\n",
    "\t\t\t\t\treq_probs.append(bi_prob[1][i])\n",
    "\t\t\tsu=sum(req_probs)\n",
    "\t\t\tfor i in range(len(req_probs)):\n",
    "\t\t\t\treq_probs[i]=req_probs[i]/su\n",
    "\t\t\trand=np.random.multinomial(5,req_probs)\n",
    "\t\t\tma=np.argmax(rand)\n",
    "\t\t\ts2out+=reqlist[ma][1]+' '\n",
    "\n",
    "\t\t\tstart=reqlist[ma][1]\n",
    "\t\ts2out=s2out.replace(' </s>','.')\n",
    "\t\tprint(s2out)\n",
    "\tprint()\n",
    "\tprint()\n",
    "\n",
    "def trigram_generator(tro_prob):\n",
    "\tprint(\"5 sentences by trigram\")\n",
    "\tprint()\n",
    "\t# trigram sentence generator\n",
    "\tfor m in range(5):\n",
    "\t\ts2out=''\n",
    "\t\tstart=['<s>','<s>']\n",
    "\t\tc=0\n",
    "\t\twhile '</s>' not in s2out :\n",
    "\t\t\treqlist=[]\n",
    "\t\t\treq_probs=[]\n",
    "\t\t\tfor i in range(len(tri_prob[0])):\n",
    "\t\t\t\tif ((c>0 and (tri_prob[0][i][0]==start[0] and tri_prob[0][i][1]==start[1])) or (c==0 and tri_prob[0][i][0]==start[0])):\n",
    "\t\t\t\t\treqlist.append(tri_prob[0][i])\n",
    "\t\t\t\t\treq_probs.append(tri_prob[1][i])\n",
    "\t\t\tsu=sum(req_probs)\n",
    "\t\t\tfor i in range(len(req_probs)):\n",
    "\t\t\t\treq_probs[i]=req_probs[i]/su\n",
    "\t\t\trand=np.random.multinomial(5,req_probs)\n",
    "\t\t\tma=np.argmax(rand)\n",
    "\t\t\tif c>0:\n",
    "\t\t\t\ts2out+=reqlist[ma][2]+' '\n",
    "\t\t\t\tstart=[start[1],reqlist[ma][2]]\n",
    "\t\t\telse:\n",
    "\t\t\t\ts2out+=reqlist[ma][1]+' '\n",
    "\t\t\t\tstart=[start[1],reqlist[ma][1]]\n",
    "\t\t\tc+=1\n",
    "\t\ts2out=s2out.replace(' </s>','.')\n",
    "\t\tprint(s2out)\n",
    "\tprint()\n",
    "\tprint()\n",
    "\n",
    "def quadgram_generator(quad_prob):\n",
    "\tprint(\"5 sentences by quadgram\")\n",
    "\tprint()\n",
    "\n",
    "\t# bigram sentence generator\n",
    "\tfor m in range(5):\n",
    "\t\ts2out=''\n",
    "\t\tstart=['<s>','<s>','<s>']\n",
    "\t\tc=0\n",
    "\t\twhile '</s>' not in s2out :\n",
    "\t\t\treqlist=[]\n",
    "\t\t\treq_probs=[]\n",
    "\t\t\tfor i in range(len(quad_prob[0])):\n",
    "#\t\t\t\tprint(start,quad_prob[0][i][:3],start==quad_prob[0][i][:3])\n",
    "\t\t\t\tif ((c==0 and quad_prob[0][i][0]==start[0]) or (c>0 and [quad_prob[0][i][0],quad_prob[0][i][1],quad_prob[0][i][2]]==start)):\n",
    "\t\t\t\t\treqlist.append(quad_prob[0][i])\n",
    "\t\t\t\t\treq_probs.append(quad_prob[1][i])\n",
    "\t\t\tsu=sum(req_probs)\n",
    "\t\t\tfor i in range(len(req_probs)):\n",
    "\t\t\t\treq_probs[i]=req_probs[i]/su\n",
    "\t\t\trand=np.random.multinomial(5,req_probs)\n",
    "\t\t\tma=np.argmax(rand)\n",
    "\n",
    "\t\t\tif c>0:\n",
    "\t\t\t\ts2out+=reqlist[ma][3]+' '\n",
    "#\t\t\t\tprint(start)\n",
    "#\t\t\t\tprint(reqlist[ma])\n",
    "\t\t\t\tstart=[start[1],start[2],reqlist[ma][3]]\n",
    "\t\t\telse:\n",
    "\t\t\t\ts2out+=reqlist[ma][1]+' '+reqlist[ma][2]+' '+reqlist[ma][3]+' '\n",
    "\t\t\t\tstart=[reqlist[ma][1],reqlist[ma][2],reqlist[ma][3]]\n",
    "\t\t\tc+=1\n",
    "\t\ts2out=s2out.replace(' </s>','.')\n",
    "\t\tprint(s2out)\n",
    "\tprint()\n",
    "\tprint()\n",
    "\n",
    "# here n is 1|2|3|4 based on which gram should be used to generate sentences.\n",
    "def generator(n):\n",
    "\tif n==1:\n",
    "\t\tunigram_generator(uni_prob)\n",
    "\telif n==2:\n",
    "\t\tbigram_generator(bi_prob)\n",
    "\telif n==3:\n",
    "\t\ttrigram_generator(tri_prob)\n",
    "\telif n==4:\n",
    "\t\tquadgram_generator(quad_prob)\n",
    "\treturn 1\n",
    "#_____________________________________________________________________________________________________\n",
    "\n",
    "\n",
    "# call the function here\n",
    "generator(1)\n",
    "generator(2)\n",
    "generator(3)\n",
    "generator(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram model perplexity : 884.6108602059542\n",
      "Bigram model perplexity : 583.5676393384372\n",
      "Trigram model perplexity : 281.6499115272768\n",
      "Quadgram model perplexity : 137.79999999998728\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def smoothing(test_n_gram,n1_gram,n1_gram_count,n0_gram_count):\n",
    "\t# Along with Add-1 smoothing\n",
    "\tle=len(n1_gram_count)\n",
    "\ttotal_n_grams=len(n1_gram)\n",
    "\tunique_grams=list(n1_gram_count.keys())\n",
    "\tprob_vec=[0 for i in range(le)]\n",
    "\td={}\n",
    "\tfor i in test_n_gram:\n",
    "\t\tif i not in d:\n",
    "\t\t\td[i]=0\n",
    "\textra=0\n",
    "\tfor i in test_n_gram:\n",
    "\t\tif i not in n1_gram_count:\n",
    "\t\t\textra+=1\n",
    "\n",
    "\tif len(n1_gram[0])==1:\n",
    "\t\tfor i in range(le):\n",
    "\t\t\tif unique_grams[i] in d:\n",
    "\t\t\t\td[unique_grams[i]]=(n1_gram_count[unique_grams[i]])/((total_n_grams+extra)*10)\n",
    "\telif len(n1_gram[0])==2:\n",
    "\t\tfor i in range(le):\n",
    "\t\t\tif unique_grams[i] in d:\n",
    "\t\t\t\td[unique_grams[i]]=(n1_gram_count[unique_grams[i]])/(n0_gram_count[(unique_grams[i][0],)])\n",
    "\telif len(n1_gram[0])==3:\n",
    "\t\tfor i in range(le):\n",
    "\t\t\tif unique_grams[i] in d:\n",
    "\t\t\t\td[unique_grams[i]]=(1+n1_gram_count[unique_grams[i]])/(le+n0_gram_count[(unique_grams[i][0],unique_grams[i][1])])\n",
    "\telif len(n1_gram[0])==4:\n",
    "\t\tfor i in range(le):\n",
    "\t\t\tif unique_grams[i] in d:\n",
    "\t\t\t\td[unique_grams[i]]=(1+n1_gram_count[unique_grams[i]])/(le+n0_gram_count[(unique_grams[i][0],unique_grams[i][1],unique_grams[i][2])])\n",
    "\tfor i in d:\n",
    "\n",
    "\t\tif (len(i)==1 and d[i]==0):\n",
    "\t\t\td[i]=.1/(total_n_grams+extra)\n",
    "\t\telif (len(i)==2 and d[i]==0):\n",
    "\t\t\td[i]=min(bi_prob[1])\n",
    "\t\telif (len(i)==3 and d[i]==0):\n",
    "\t\t\td[i]=min(tri_prob[1])\n",
    "\t\telse:\n",
    "\t\t\td[i]=min(quad_prob[1])\n",
    "\treturn(d)\n",
    "\n",
    "\n",
    "#_____________________________________________________________________________________________________\n",
    "def smoothing_2(test_data):\n",
    "\tfinal_data=data_cleaner(test_data)\n",
    "\ttest_unigrams=[]\n",
    "\ttest_bigrams=[]\n",
    "\ttest_trigrams=[]\n",
    "\ttest_quadgrams=[]\n",
    "\tfor i in final_data:\n",
    "\t\ts = i.lower()\n",
    "\t\ts = re.sub(r\"[^a-zA-Z0-9'\\s]\", ' ', s)\n",
    "\t\ts='<s> '+s+' </s>'\n",
    "\t\ttokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "\t\ttest_unigrams+=list(ngrams(tokens, 1))\n",
    "\t\ttest_bigrams+=list(ngrams(tokens, 2))\n",
    "\t\ttest_trigrams+=list(ngrams(tokens, 3))\n",
    "\t\ttest_quadgrams+=list(ngrams(tokens, 4))\n",
    "\tk1=smoothing(test_unigrams,unigrams,unigram_count,unigram_count)\n",
    "\tk2=smoothing(test_bigrams,bigrams,bigram_count,unigram_count)\n",
    "\tk3=smoothing(test_trigrams,trigrams,trigram_count,bigram_count)\n",
    "\tk4=smoothing(test_quadgrams,quadgrams,quadgram_count,trigram_count)\n",
    "\treturn (k1,k2,k3,k4)\n",
    "def perplexity(k):    \n",
    "\tp=1\n",
    "\ts=0\n",
    "\ttotal_len=len(k)\n",
    "\tfor i in k:\n",
    "\t\ts+=k[i]\n",
    "\tfor i in k:\n",
    "\t\tk[i]=5*k[i]\n",
    "\t\tp=p*((k[i])**(1/total_len))\n",
    "\tperp=(1/p)\n",
    "\treturn(perp)\n",
    "\n",
    "print(\"Unigram model perplexity : \" + str(perplexity(smoothing_2(test_data)[0])))\n",
    "print(\"Bigram model perplexity : \" + str(perplexity(smoothing_2(test_data)[1])))\n",
    "print(\"Trigram model perplexity : \" + str(perplexity(smoothing_2(test_data)[2])))\n",
    "print(\"Quadgram model perplexity : \" + str(perplexity(smoothing_2(test_data)[3])))\n",
    "# this calculation would take time around 5 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 5242\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=None, filters=[], lower=True, split=\" \") # data is already filtered, so no need to filter any thing else\n",
    "tokenizer.fit_on_texts(data_train) # fitting indices to words\n",
    "word_index = tokenizer.word_index # word -> index\n",
    "index_word = {} # index -> word\n",
    "for word in word_index:\n",
    "    index_word[word_index[word]] = word\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) # Vocabulary size\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "data_train = tokenizer.texts_to_sequences(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_model:\n",
    "    def __init__(self, data, tokenizer, word_index, index_word, max_sequence_len=15, hiddenlayer=\"Vanilla RNN\", epochs=20, batch_size=256):\n",
    "        self.data= data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sentences = data\n",
    "        self.wordtoindex = word_index\n",
    "        self.indextoword = index_word\n",
    "        self.max_sequence_len = max_sequence_len\n",
    "        self.vocab = len(self.wordtoindex) + 1\n",
    "        self.hidden = hiddenlayer\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def train(self):\n",
    "        self.prepare_data()\n",
    "        self.model = self.model_create()\n",
    "        \n",
    "    def evaluate(self, data):\n",
    "        data1 = self.tokenizer.texts_to_sequences(data)\n",
    "        input_sequences = []\n",
    "        for line in data1:\n",
    "            for i in range(1, len(line)):\n",
    "                n_gram_sequence = line[:i+1]\n",
    "                input_sequences.append(n_gram_sequence)\n",
    "        input_sequences = np.array(pad_sequences(input_sequences,maxlen = self.max_sequence_len, padding='pre'))\n",
    "        X, y = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "        y = to_categorical(y, num_classes=self.vocab)\n",
    "        loss = self.model.evaluate(X,y, verbose=0)\n",
    "        return np.exp(loss)\n",
    "        \n",
    "        \n",
    "    def prepare_data(self):\n",
    "        input_sequences = []\n",
    "        for line in self.data:\n",
    "            for i in range(1, len(line)):\n",
    "                n_gram_sequence = line[:i+1]\n",
    "                input_sequences.append(n_gram_sequence)\n",
    "        input_sequences = np.array(pad_sequences(input_sequences,maxlen = self.max_sequence_len, padding='pre'))\n",
    "        self.X, y = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "        self.y = to_categorical(y, num_classes=self.vocab)\n",
    "\n",
    "    def model_create(self):\n",
    "        input_len = self.max_sequence_len - 1\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(self.vocab, 128, input_length=input_len))\n",
    "        if self.hidden == \"Vanilla RNN\":\n",
    "            model.add(SimpleRNN(256))\n",
    "        elif self.hidden == \"LSTM\":\n",
    "            model.add(LSTM(256))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(self.vocab, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "        print(model.summary())\n",
    "        model.fit(self.X, self.y, epochs=self.epochs, verbose=1, batch_size=self.batch_size)\n",
    "        return model\n",
    "        \n",
    "    def generate_text(self, n_sent):\n",
    "        in_text, result = [\"<s>\"], \"\"\n",
    "        sent_len=1\n",
    "        started = 0\n",
    "        while (n_sent>0):\n",
    "            encoded = np.array(self.tokenizer.texts_to_sequences([in_text])[0])\n",
    "            encoded = pad_sequences([encoded], maxlen= self.max_sequence_len-1, padding='pre')\n",
    "            probs = list(np.transpose(self.model.predict_proba(encoded, verbose=0))[:,0])\n",
    "            a=sum(probs)*1.1\n",
    "            yhat=[i/a for i in probs]\n",
    "            out = list(np.random.multinomial(2,yhat))\n",
    "            i = out.index(max(out))\n",
    "            out_word = self.indextoword[i]\n",
    "            if out_word==\"</s>\" or sent_len == self.max_sequence_len:\n",
    "                out_word = \".\"\n",
    "                in_text = [\"<s>\"]\n",
    "                n_sent-=1\n",
    "                sent_len=1\n",
    "                result = result + out_word\n",
    "                started=0\n",
    "            else:\n",
    "                in_text+=[out_word]\n",
    "                sent_len+=1\n",
    "                if not started:\n",
    "                    started = 1\n",
    "                    out_word = out_word.capitalize()\n",
    "                result = result + \" \" + out_word\n",
    "        return result.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 14, 128)           671104    \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 256)               98560     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5243)              1347451   \n",
      "=================================================================\n",
      "Total params: 2,117,115\n",
      "Trainable params: 2,117,115\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 153570 samples\n",
      "Epoch 1/10\n",
      "153570/153570 [==============================] - 14s 90us/sample - loss: 5.5566\n",
      "Epoch 2/10\n",
      "153570/153570 [==============================] - 12s 80us/sample - loss: 4.6090s - loss:\n",
      "Epoch 3/10\n",
      "153570/153570 [==============================] - 13s 84us/sample - loss: 4.2470\n",
      "Epoch 4/10\n",
      "153570/153570 [==============================] - 12s 81us/sample - loss: 3.9972s - loss: 3.9\n",
      "Epoch 5/10\n",
      "153570/153570 [==============================] - 12s 79us/sample - loss: 3.7987\n",
      "Epoch 6/10\n",
      "153570/153570 [==============================] - 12s 77us/sample - loss: 3.6334\n",
      "Epoch 7/10\n",
      "153570/153570 [==============================] - 12s 77us/sample - loss: 3.4891\n",
      "Epoch 8/10\n",
      "153570/153570 [==============================] - 12s 80us/sample - loss: 3.3645s - loss: 3.3\n",
      "Epoch 9/10\n",
      "153570/153570 [==============================] - 12s 79us/sample - loss: 3.2554\n",
      "Epoch 10/10\n",
      "153570/153570 [==============================] - 12s 77us/sample - loss: 3.1593\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = neural_model(data=data_train, tokenizer=tokenizer, word_index=word_index, index_word=index_word, max_sequence_len=15, hiddenlayer=\"Vanilla RNN\", epochs=10, batch_size=256)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 14, 128)           671104    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5243)              1347451   \n",
      "=================================================================\n",
      "Total params: 2,412,795\n",
      "Trainable params: 2,412,795\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 153570 samples\n",
      "Epoch 1/10\n",
      "153570/153570 [==============================] - 14s 92us/sample - loss: 5.5799\n",
      "Epoch 2/10\n",
      "153570/153570 [==============================] - 13s 86us/sample - loss: 4.7835\n",
      "Epoch 3/10\n",
      "153570/153570 [==============================] - 12s 78us/sample - loss: 4.5042\n",
      "Epoch 4/10\n",
      "153570/153570 [==============================] - 12s 80us/sample - loss: 4.3201\n",
      "Epoch 5/10\n",
      "153570/153570 [==============================] - 12s 79us/sample - loss: 4.1706\n",
      "Epoch 6/10\n",
      "153570/153570 [==============================] - 13s 82us/sample - loss: 4.0391\n",
      "Epoch 7/10\n",
      "153570/153570 [==============================] - 12s 80us/sample - loss: 3.9227\n",
      "Epoch 8/10\n",
      "153570/153570 [==============================] - 13s 82us/sample - loss: 3.8153\n",
      "Epoch 9/10\n",
      "153570/153570 [==============================] - 13s 83us/sample - loss: 3.7179\n",
      "Epoch 10/10\n",
      "153570/153570 [==============================] - 12s 81us/sample - loss: 3.6304\n"
     ]
    }
   ],
   "source": [
    "model_lstm = neural_model(data=data_train, tokenizer=tokenizer, word_index=word_index, index_word=index_word, max_sequence_len=15, hiddenlayer=\"LSTM\", epochs=10, batch_size=256)\n",
    "model_lstm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla RNN model\n",
      "\n",
      "\n",
      "You have to you have to know mr trump they re in a bridge. And i have a great smaller country on the world. And we re going to come in legally. We have four or with the veterans that add to the republicans. They have polls they need people at the united states.\n",
      "\n",
      "\n",
      "LSTM model\n",
      "\n",
      "\n",
      "Thank you something. I will tell you. The highest tour we will just i have iran to be america to have. I think he s a fantastic negotiator. I said we re bringing a poll.\n"
     ]
    }
   ],
   "source": [
    "print(\"Vanilla RNN model\\n\\n\")\n",
    "print(model.generate_text(5))\n",
    "print(\"\\n\\nLSTM model\\n\\n\")\n",
    "print(model_lstm.generate_text(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla RNN model:\n",
      "88.60448136480957\n",
      "\n",
      "LSTM model:\n",
      "83.86604432397661\n"
     ]
    }
   ],
   "source": [
    "print(\"Vanilla RNN model:\")\n",
    "print(model.evaluate(data_test))\n",
    "print(\"\\nLSTM model:\")\n",
    "print(model_lstm.evaluate(data_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both The Neural Approach and Natural language model approach generate good senteces except the unigram one, based on the perplexities neural dominates the other because of taking into consideration more that 4 words in a window and learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
